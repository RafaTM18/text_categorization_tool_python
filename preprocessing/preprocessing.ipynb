{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /home/rafael/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/rafael/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package rslp to /home/rafael/nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n"
    }
   ],
   "source": [
    "#Libraries to manage text data \n",
    "\n",
    "## SKLearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## NLTK\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')\n",
    "stemmer_pt = RSLPStemmer()\n",
    "stemmer_en = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Python\n",
    "import string\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#Libraries to manage the file system\n",
    "import os\n",
    "\n",
    "#Other libraries\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import joblib\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_pt = set({})\n",
    "stopwords_en = set({})\n",
    "path_stop_pt = './stopPort.txt'\n",
    "path_stop_en = './stopIngl.txt'\n",
    "\n",
    "if(os.path.exists(path_stop_pt) and os.path.exists(path_stop_en)): \n",
    "    with open(path_stop_pt) as file_stop_pt:\n",
    "        for line in file_stop_pt.readlines():\n",
    "            stopwords_pt.add(line.strip())\n",
    "    with open(path_stop_en) as file_stop_en:\n",
    "        for line in file_stop_en.readlines():\n",
    "            stopwords_en.add(line.strip())\n",
    "else: \n",
    "    stopwords_pt = set(stopwords.words('portuguese'))\n",
    "    stopwords_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to Tokenize and Clean the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['pre-processing_steps']['lower_case'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'language': 'en',\n 'remove_stopwords': True,\n 'remove_punctuation': True,\n 'convert_numbers': True,\n 'remove_numbers': True,\n 'simplification': True,\n 'simplification_type': 'stemming',\n 'lower_case': True}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "config['pre-processing_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor(**config['pre-processing_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Language: en\nRemove stopwords: True\nRemove puctuation: True\nConvert numbers: True\nRemove numbers: True\nSimplification: True\nSimplification type: stemming\nLower case: True\n"
    }
   ],
   "source": [
    "print('Language:', text_preprocessor.language)\n",
    "print('Remove stopwords:', text_preprocessor.remove_stopwords)\n",
    "print('Remove puctuation:', text_preprocessor.remove_punctuation)\n",
    "print('Convert numbers:', text_preprocessor.convert_numbers)\n",
    "print('Remove numbers:', text_preprocessor.remove_numbers)\n",
    "print('Simplification:', text_preprocessor.simplification)\n",
    "print('Simplification type:', text_preprocessor.simplification_type)\n",
    "print('Lower case:', text_preprocessor.lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(object): \n",
    "    \n",
    "    def __init__(self, language='en', remove_stopwords=True, remove_punctuation=True, \n",
    "                 convert_numbers = True, remove_numbers = False, simplification=True, \n",
    "                 simplification_type='lemmatization', lower_case = True): \n",
    "        self.language = language\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.convert_numbers = convert_numbers\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.simplification = simplification\n",
    "        self.simplification_type = simplification_type \n",
    "        self.lower_case = lower_case\n",
    "\n",
    "\n",
    "    # Complete function to standardize the text\n",
    "    def text_cleaner(self, text): \n",
    "        new_text = ''\n",
    "        stopwords = None \n",
    "\n",
    "        if self.language == 'en':\n",
    "            stopwords = stopwords_en \n",
    "        else:\n",
    "            stopwords = stopwords_pt\n",
    "\n",
    "        if self.lower_case == True: \n",
    "            text = text.lower()\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        if self.remove_stopwords == True:\n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token in stopwords:\n",
    "                    continue \n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "\n",
    "        if self.remove_punctuation == True: \n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token in string.punctuation:\n",
    "                    continue \n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "        \n",
    "        if self.remove_numbers == True:\n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token.isnumeric():\n",
    "                    continue\n",
    "                new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "        \n",
    "        if self.convert_numbers == True: \n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token.isnumeric():\n",
    "                    new_tokens.append(\"0\"*len(token))\n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "\n",
    "        if self.simplification == True: \n",
    "            new_tokens = []\n",
    "            if self.language == 'en': \n",
    "                if self.simplification_type  == 'lemmatization':\n",
    "                    for token in tokens: \n",
    "                        new_tokens.append(lemmatizer.lemmatize(token))\n",
    "                elif self.simplification_type  == 'stemming':\n",
    "                    for token in tokens: \n",
    "                        new_tokens.append(stemmer_en.stem(token))\n",
    "                else: \n",
    "                    raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "            elif self.language == 'pt':\n",
    "                for token in tokens: \n",
    "                        new_tokens.append(stemmer_en.stem(token))\n",
    "            else: \n",
    "                raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "            tokens = new_tokens\n",
    "\n",
    "        return ' '.join(tokens).strip()\n",
    "\n",
    "\n",
    "    #Just a simple tokenizer\n",
    "    def tokenizer(self, text):\n",
    "        text = text.lower()\n",
    "        lista_alfanumerica = []\n",
    "\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            if token in string.punctuation:\n",
    "                continue \n",
    "            if token in stopwords_en: \n",
    "                continue\n",
    "            if token.isnumeric():\n",
    "                token = \"0\"*len(token)\n",
    "\n",
    "            lista_alfanumerica.append(token)\n",
    "        return lista_alfanumerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(self, text):\n",
    "    text = text.lower()\n",
    "    lista_alfanumerica = []\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation:\n",
    "            continue \n",
    "        if token in stopwords_en: \n",
    "            continue\n",
    "        if token.isnumeric():\n",
    "            token = \"0\"*len(token)\n",
    "\n",
    "        lista_alfanumerica.append(token)\n",
    "    return lista_alfanumerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(self, text, language='en', remove_stopwords=True, remove_punctuation=True, \n",
    "                 convert_numbers = True, remove_numbers = False, simplification=True, \n",
    "                 simplification_type='lemmatization', lower_case = True): \n",
    "    new_text = ''\n",
    "    stopwords = None \n",
    "    if self.language == 'en':\n",
    "        stopwords = stopwords_en \n",
    "    else:\n",
    "        stopwords = stopwords_pt\n",
    "\n",
    "    if lower_case == True: \n",
    "        text = text.lower()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        new_tokens = []\n",
    "        for token in tokens: \n",
    "            if token in stopwords:\n",
    "                continue \n",
    "            else: \n",
    "                new_tokens.append(token)\n",
    "        tokens = new_tokens \n",
    "\n",
    "    if remove_punctuation == True: \n",
    "        new_tokens = []\n",
    "        for token in tokens: \n",
    "            if token in string.punctuation:\n",
    "                continue \n",
    "            else: \n",
    "                new_tokens.append(token)\n",
    "        tokens = new_tokens \n",
    "    \n",
    "    if convert_numbers == True: \n",
    "        new_tokens = []\n",
    "        for token in tokens: \n",
    "            if token.isnumeric():\n",
    "                token = \"0\"*len(token)\n",
    "            new_tokens.append(token)\n",
    "        tokens = new_tokens \n",
    "\n",
    "    if remove_numbers == True: \n",
    "        new_tokens = []\n",
    "        for token in tokens: \n",
    "            if token.isnumeric():\n",
    "                continue\n",
    "            new_tokens.append(token)\n",
    "        tokens = new_tokens \n",
    "\n",
    "    if simplification == True: \n",
    "        new_tokens = []\n",
    "        if language == 'en': \n",
    "            if simplification_type  == 'lemmatization':\n",
    "                for token in tokens: \n",
    "                    new_tokens.append(lemmatizer.lemmatize(token))\n",
    "            elif simplification_type  == 'stemming':\n",
    "                for token in tokens: \n",
    "                    new_tokens.append(stemmer_en.stem(token))\n",
    "            else: \n",
    "                raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "        elif language == 'pt':\n",
    "            for token in tokens: \n",
    "                    new_tokens.append(stemmer_en.stem(token))\n",
    "        else: \n",
    "            raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "        tokens = new_tokens\n",
    "\n",
    "    return ' '.join(tokens).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Save and Load the Presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representation(representation, path): \n",
    "        joblib.dump(representation,path)\n",
    "        \n",
    "def load_representation(path): \n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to Generate a Standard Representation for Different Space Vector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredRepresentation():\n",
    "\n",
    "    def __init__(self, doc_vectors=None, class_vectors=None, vocabulary=None): \n",
    "        self.text_vectors = doc_vectors\n",
    "        self.class_vectors = class_vectors \n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "  \n",
    "    def save_arff(self, name, path, non_sparse_format = False):\n",
    "        num_docs = self.text_vectors.shape[0]\n",
    "        num_attrs = self.text_vectors.shape[1]\n",
    "        with open(path, 'w') as arff: \n",
    "            #Writting the relation\n",
    "            arff.write(f'@relation {name}\\n\\n')\n",
    "            \n",
    "            #Writting the attributes\n",
    "            if self.vocabulary == None: \n",
    "                for attr in range(num_attrs): \n",
    "                    arff.write(f'@ATTRIBUTE dim{attr + 1} NUMERIC\\n')\n",
    "            else: \n",
    "                sorted_vocabulary = sorted(self.vocabulary.items(), key=lambda x: x[1])\n",
    "                for attr in range(num_attrs): \n",
    "                    arff.write(f'@ATTRIBUTE {sorted_vocabulary[attr][0]} NUMERIC\\n')\n",
    "            \n",
    "            #Writting the class names\n",
    "            arff.write('@ATTRIBUTE att_class ' + '{\"' + '\",\"'.join(self.class_vectors.unique()) + '\"}\\n\\n')\n",
    "\n",
    "\n",
    "            #Writting the data\n",
    "            arff.write('@data\\n\\n')\n",
    "\n",
    "            if non_sparse_format == False: \n",
    "                for doc in range(num_docs):\n",
    "                    vector = self.text_vectors[doc]\n",
    "                    if type(vector) == scipy.sparse.csr.csr_matrix: \n",
    "                        vector = self.text_vectors[doc].toarray()[0]\n",
    "                    str_vec = ''\n",
    "                    for i in range(vector.shape[0]): \n",
    "                        str_vec += str(vector[i]) + ','\n",
    "                    classe = self.class_vectors[doc]\n",
    "                    arff.write(str_vec + '\"' + classe + '\"\\n') \n",
    "            else: \n",
    "                for doc in range(num_docs):\n",
    "                    vector = self.text_vectors[doc]\n",
    "                    if type(vector) == scipy.sparse.csr.csr_matrix: \n",
    "                        vector = self.text_vectors[doc].toarray()[0]\n",
    "                    str_vec = ''\n",
    "                    for i in range(vector.shape[0]): \n",
    "                        if vector[i] > 0: \n",
    "                            str_vec += f'{i} {str(vector[i])},'\n",
    "                    classe = self.class_vectors[doc]\n",
    "                    arff.write('{' + str_vec + str(num_attrs) + ' \"' + classe + '\"}\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes to Generate Vector Space Model Based Representaions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words or Bag-of-N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySparseVSM: \n",
    "\n",
    "    def __init__(self, weight='tf', n_grams=1):\n",
    "        self.vectorizer = None \n",
    "        if(weight == 'tf'):\n",
    "            self.vectorizer = CountVectorizer(min_df=2, ngram_range=(1, n_grams), dtype=np.uint8)\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, n_grams), dtype=np.uint8)\n",
    "\n",
    "        self.structured_representation = None\n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.structured_representation = StructuredRepresentation(self.vectorizer.fit_transform(texts), classes, self.vectorizer.vocabulary_)\n",
    "        return self.structured_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Dimensional Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuperClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowDimensionalRepresentation(object):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = None, num_threads=1, min_count = 2, window_size = 5): \n",
    "        __metaclass__  = ABCMeta\n",
    "        self.dim_size = dim_size\n",
    "        self.model = model \n",
    "        self.num_threads = num_threads \n",
    "        self.min_count = min_count\n",
    "        self.window_size = window_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_representation(self, texts, classes): \n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWord2Vec (LowDimensionalRepresentation):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = 'skip-gram', method='average', num_threads=1, min_count = 2, window_size = 5): \n",
    "        super(MyWord2Vec,self).__init__(dim_size,model,num_threads,min_count,window_size)\n",
    "        self.language_model = None \n",
    "        self.cg = None \n",
    "\n",
    "    def build_model(self, texts):\n",
    "        language_model = None\n",
    "\n",
    "        sg = 0\n",
    "        if self.model == 'cbow' : \n",
    "            language_model = gensim.models.Word2Vec\n",
    "        elif self.model == 'skip-gram': \n",
    "            language_model = gensim.models.Word2Vec\n",
    "            sg = 1\n",
    "        #elif self.model == 'glove': \n",
    "            #self.language_model = gensim.models.Word2Vec(list_tokens_texts,min_count=min_count,window=window_size, size=dim_size, workers=num_threads)\n",
    "        elif self.model == 'fasttext': \n",
    "            language_model = gensim.models.FastText\n",
    "        else: \n",
    "            raise ValueError('Unsuported language model. Please, use language model = {\"cbow\",\"skip-gram\"\",\"glove\"\",\"fasttext\"}.')\n",
    "\n",
    "        list_tokens_texts = texts.apply(self.tokenizer)\n",
    "        self.language_model = language_model(list_tokens_texts,sg=sg, min_count=self.min_count,window=self.window_size, size=self.dim_size, workers=self.num_threads)\n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.build_language_model(texts)\n",
    "        matrix = np.zeros((len(texts),self.dim_size))\n",
    "\n",
    "        for i in range(len(texts)):\n",
    "            tokens = self.tokenizer(texts.iloc[i])\n",
    "            matrix[i] = self.sum_vectors(tokens)\n",
    "\n",
    "\n",
    "        self.structured_representation = StructuredRepresentation(matrix, classes, list(self.language_model.wv.vocab))\n",
    "        return self.structured_representation\n",
    "\n",
    "    def tokenizer(self,text):\n",
    "        text = text.lower()\n",
    "        lista_alfanumerica = []\n",
    "\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            if token in string.punctuation:\n",
    "                continue \n",
    "            if token in stopwords_en: \n",
    "                continue\n",
    "            if token.isnumeric():\n",
    "                token = \"0\"*len(token)\n",
    "\n",
    "            lista_alfanumerica.append(token)\n",
    "        return lista_alfanumerica\n",
    "\n",
    "    def sum_vectors(self,lista_tokens): \n",
    "        vetor_combinado = np.zeros(self.dim_size)\n",
    "        for token in lista_tokens: \n",
    "            try:\n",
    "                vetor_combinado += self.language_model.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                if token.isnumeric():\n",
    "                    token = \"0\"*len(token)\n",
    "                    vetor_combinado += self.language_model.wv.get_vector(token)\n",
    "                else:\n",
    "                    token = \"0\"*len(token)\n",
    "                    vetor_combinado += self.language_model.wv.get_vector(\"unknown\")\n",
    "        return vetor_combinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDoc2Vec (LowDimensionalRepresentation):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = 'dm', method='average', num_threads=4, alpha = 0.025, min_alpha=0.0001, num_max_epochs = 2000,min_count = 1, window_size = 5): \n",
    "        super(MyDoc2Vec,self).__init__(dim_size,model,num_threads,min_count,window_size)\n",
    "        \n",
    "        self.num_threads = num_threads\n",
    "        self.alpha = alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.num_max_epochs = num_max_epochs\n",
    "        self.model = model\n",
    "\n",
    "        self.dm = -1\n",
    "        if model == 'dbow':\n",
    "            self.dm = 0\n",
    "        elif model == 'dm':\n",
    "            self.dm = 1\n",
    "        elif model != 'both':\n",
    "            raise ValueError('Unsuported model. Please, use model = {\"dm\",\"dbow\"}.')\n",
    "        \n",
    "        self.dm_mean = 1\n",
    "        if method == 'average': \n",
    "            self.dm_concat = 0\n",
    "        elif method == 'concat':\n",
    "            self.dm_concat = 1\n",
    "        else:\n",
    "            raise ValueError('Unsuported method. Please, use method = {\"concat\"\",\"average\"}.')\n",
    "        \n",
    "        #standard parameters\n",
    "        self.hs = 0\n",
    "        self.dbow_words = 0\n",
    "        \n",
    "    \n",
    "    def build_model(self, texts): \n",
    "        \n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(texts)]\n",
    "        if self.model == 'dm' or self.model == 'dbow': \n",
    "            model = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=self.dm, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "            model.build_vocab(tagged_data)\n",
    "            model.train(tagged_data,total_examples=model.corpus_count,epochs=model.iter)\n",
    "            \n",
    "            #Reduce memory usage\n",
    "            model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "            matrix = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(model.corpus_count): \n",
    "                matrix[i] = model.docvecs[str(i)]\n",
    "            return matrix\n",
    "        elif self.model == 'both': \n",
    "            modelDM = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=1, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "            modelDBOW = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=0, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "                        \n",
    "            modelDM.build_vocab(tagged_data)\n",
    "            modelDBOW.build_vocab(tagged_data)\n",
    "\n",
    "            modelDM.train(tagged_data,total_examples=modelDM.corpus_count,epochs=modelDM.iter)\n",
    "            modelDBOW.train(tagged_data,total_examples=modelDBOW.corpus_count,epochs=modelDBOW.iter)\n",
    "            \n",
    "            #Reduce memory usage\n",
    "            modelDM.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "            modelDBOW.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "\n",
    "            matrixDM = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(modelDM.corpus_count): \n",
    "                matrixDM[i] = modelDM.docvecs[str(i)]\n",
    "            matrixDBOW = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(modelDBOW.corpus_count): \n",
    "                matrixDBOW[i] = modelDBOW.docvecs[str(i)]\n",
    "            \n",
    "            matrix = np.concatenate([matrixDM, matrixDBOW], axis=1)\n",
    "            return matrix\n",
    "\n",
    "  \n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.structured_representation = StructuredRepresentation(self.build_model(texts), classes, None)\n",
    "        return self.structured_representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Área de Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  file_name                                               text  \\\n0   126.txt  Rhetorical (Rhet) is a programming / knowledge...   \n1     5.txt  Reduction is the operation of transforming a p...   \n2    48.txt  For years, researchers have used knowledge-int...   \n3    81.txt  Proceedings of a workshop held in conjunction ...   \n4    25.txt  The Medication Advisor is the latest project o...   \n\n                     class  \n0  ArtificiallIntelligence  \n1  ArtificiallIntelligence  \n2  ArtificiallIntelligence  \n3  ArtificiallIntelligence  \n4  ArtificiallIntelligence  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>126.txt</td>\n      <td>Rhetorical (Rhet) is a programming / knowledge...</td>\n      <td>ArtificiallIntelligence</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.txt</td>\n      <td>Reduction is the operation of transforming a p...</td>\n      <td>ArtificiallIntelligence</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48.txt</td>\n      <td>For years, researchers have used knowledge-int...</td>\n      <td>ArtificiallIntelligence</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>81.txt</td>\n      <td>Proceedings of a workshop held in conjunction ...</td>\n      <td>ArtificiallIntelligence</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25.txt</td>\n      <td>The Medication Advisor is the latest project o...</td>\n      <td>ArtificiallIntelligence</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "import pandas as pd \n",
    "path = '/home/rafael/Área de Trabalho/Produção Científica/JIPM 2020/Revisao V1/complete_texts_csvs/CSTR.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-a06689b9c173>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-a06689b9c173>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    myDoc2Vec = MyDoc2Vec(dim_size=100, num_max_epochs=100, min_count=5)pre-processing\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "myDoc2Vec = MyDoc2Vec(dim_size=100, num_max_epochs=100, min_count=5)\n",
    "representation = myDoc2Vec.build_representation(df['text'].apply(text_cleaner),df['class'])\n",
    "representation.save_arff('teste', 'teste.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dicionario (versão completa)\n",
    "config = {}\n",
    "config['csvs_diretory'] = '/home/rafael/Área de Trabalho/Produção Científica/JIPM 2020/Revisao V1/complete_texts_csvs'\n",
    "config['output_directory'] = './teste'\n",
    "config['text_column'] = 'text'\n",
    "config['class_column'] = 'class'\n",
    "config['pre-processing'] = True\n",
    "config['pre-processing_steps'] = {'language' : 'en', 'remove_stopwords' : True, 'remove_punctuation' : True, \n",
    "                 'convert_numbers' : True, 'remove_numbers' : True, 'simplification' : True, \n",
    "                 'simplification_type' : 'lemmatization', 'lower_case' : True}\n",
    "config['sparse_representation'] = {'use': True, 'n-grams' : [1], 'term-weights' : ['tf', 'tf-idf']}\n",
    "config['low-dimension_representation'] = {'use' : True, 'types' : ['doc2vec', 'word2vec'] ,\n",
    "                                          'doc2vec_config' : {'models': ['dm', 'dbow', 'both'], 'methods' : ['average','concat'], \n",
    "                                          'num_threads': 4, 'alpha' : 0.025, 'min_alpha' : 0.001,\n",
    "                                          'num_max_epochs' : [1, 3, 100, 1000], 'min_count' : 1, 'window_sizes' : [5, 8, 10], \n",
    "                                          'num_dimensions' : [25,  50, 100, 500, 1000] }}\n",
    "config['save-arff'] = True \n",
    "config['save-binary'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dicionario (versão simplificada)\n",
    "config = {}\n",
    "config['csvs_diretory'] = '/home/rafael/Área de Trabalho/Produção Científica/JIPM 2020/Revisao V1/complete_texts_csvs_temp/temp1'\n",
    "config['output_directory'] = './teste'\n",
    "config['text_column'] = 'text'\n",
    "config['class_column'] = 'class'\n",
    "config['pre-processing'] = True\n",
    "config['pre-processing_steps'] = {'language' : 'en', 'remove_stopwords' : True, 'remove_punctuation' : True, \n",
    "                 'convert_numbers' : True, 'remove_numbers' : True, 'simplification' : True, \n",
    "                 'simplification_type' : 'lemmatization', 'lower_case' : True}\n",
    "config['sparse_representation'] = {'use': False, 'n-grams' : [1], 'term-weights' : ['tf', 'tf-idf']}\n",
    "config['low-dimension_representation'] = {'use' : True, 'types' : ['doc2vec', 'word2vec'] ,\n",
    "                                          'doc2vec_config' : {'models': ['dm', 'both'], 'methods' : ['average','concat'], \n",
    "                                          'num_threads': 4, 'alpha' : 0.025, 'min_alpha' : 0.0001,\n",
    "                                          'num_max_epochs' : [100], 'min_count' : 2, 'window_sizes' : [5, 10], \n",
    "                                          'num_dimensions' : [100, 200]}}\n",
    "config['save-arff'] = True \n",
    "config['save-binary'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Json\n",
    "with open('config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile, indent=4, ensure_ascii=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Json\n",
    "with open('config.json') as json_file:\n",
    "    new_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'csvs_diretory': '/home/rafael,teste',\n 'text_column': 'text',\n 'class_column': 'class',\n 'pre-processing': True,\n 'pre-processing_steps': {'language': 'en',\n  'remove_stopwords': True,\n  'remove_punctuation': True,\n  'convert_numbers': True,\n  'remove_numbers': True,\n  'simplification': True,\n  'simplification_type': 'lemmatization',\n  'lower_case': True},\n 'sparse_representation': {'use': True,\n  'n-grams': [1],\n  'term-weight': ['tf', 'tf-idf']},\n 'low-dimension_representation': {'use': True,\n  'types': ['doc2vec', 'word2vec'],\n  'doc2vec_config': {'model': ['dm', 'dbow'],\n   'methods': ['average', 'concat'],\n   'num_threads': 4,\n   'alpha': 0.025,\n   'min_alpha': 0.0001,\n   'num_max_epochs': [100, 200, 500, 1000],\n   'min_count': 2,\n   'window_size': [5, 10, 15]}}}"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path, text_column, class_column): \n",
    "    df = pd.read_csv(os.path.join(directory,csv_file))\n",
    "    df = df.dropna()\n",
    "    texts = df[config['text_column']]\n",
    "    classes = df[config['class_column']]\n",
    "    return texts, classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_name(name, representation_type, config): \n",
    "    final_name = f'{name}_{representation_type}'\n",
    "    for item in config.items(): \n",
    "        final_name += f'_{item[0]}={item[1]}'\n",
    "    \n",
    "    return final_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_representation(config, rep_builder, name_builder ,parameters, dataset_name, non_sparse_format): \n",
    "    representation = rep_builder.build_representation(texts,classes)\n",
    "    representation_name = build_name(dataset_name, name_builder, parameters)\n",
    "    if config['save-arff'] == True: \n",
    "        representation.save_arff(representation_name, os.path.join(config['output_directory'], representation_name + '.arff'),                                                          non_sparse_format = non_sparse_format)\n",
    "    if config['save-binary'] == True: \n",
    "        save_representation(representation, os.path.join(config['output_directory'], representation_name + '.rep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=============================================\n=============================================\nDataset:  webkb-parsed\nPreprocessing text collection\n=============================================\nDoc2Vec\nModel: dm\nMethod: average\nNum. Max Epochs: 100\nWindow Size: 5\nNum. Dimensions: 100\nEpochs Model:  100\nNum. Dimensions: 200\nEpochs Model:  100\nWindow Size: 10\nNum. Dimensions: 100\nEpochs Model:  100\nNum. Dimensions: 200\nEpochs Model:  100\nMethod: concat\nNum. Max Epochs: 100\nWindow Size: 5\nNum. Dimensions: 100\nEpochs Model:  100\nNum. Dimensions: 200\nEpochs Model:  100\nWindow Size: 10\nNum. Dimensions: 100\nEpochs Model:  100\nNum. Dimensions: 200\nEpochs Model:  100\nModel: both\nMethod: average\nNum. Max Epochs: 100\nWindow Size: 5\nNum. Dimensions: 100\nNum. Dimensions: 200\nWindow Size: 10\nNum. Dimensions: 100\nNum. Dimensions: 200\nMethod: concat\nNum. Max Epochs: 100\nWindow Size: 5\nNum. Dimensions: 100\nNum. Dimensions: 200\nWindow Size: 10\nNum. Dimensions: 100\nNum. Dimensions: 200\nProcess Concluded!!\n"
    }
   ],
   "source": [
    "#Processing the JSON\n",
    "\n",
    "#Getting the directory of the csvs and listing the csvs \n",
    "text_preprocessor = TextPreprocessor(**config['pre-processing_steps'])\n",
    "directory = config['csvs_diretory']\n",
    "for csv_file in sorted(os.listdir(directory)):\n",
    "    dataset_name = csv_file[:csv_file.rindex('.')]\n",
    "    print('=============================================')\n",
    "    print('=============================================')\n",
    "    print('Dataset: ', dataset_name)\n",
    "    \n",
    "    # Loading the CSVs and getting the column of the texts and the classes\n",
    "    texts, classes = load_csv(os.path.join(directory,csv_file), config['text_column'], config['class_column'])\n",
    "\n",
    "    #Pre-prossing texts\n",
    "    if config['pre-processing'] == True: \n",
    "        print('Preprocessing text collection')\n",
    "        texts = texts.apply(text_preprocessor.text_cleaner)\n",
    "    \n",
    "    #Processing sparse representations\n",
    "    if config['sparse_representation']['use'] == True: \n",
    "        print('=============================================')\n",
    "        print('Sparse Representation')\n",
    "        for ngram in config['sparse_representation']['n-grams']: \n",
    "            print('N-gram: ', ngram)\n",
    "            for term_weight in config['sparse_representation']['term-weights']: \n",
    "                print('Term-weight: ', term_weight)\n",
    "                parameters = {'term-weight' : term_weight, 'n-grams' : ngram}\n",
    "                mySparseVSM = MySparseVSM(weight=term_weight, n_grams=ngram)\n",
    "                build_and_save_representation(config, mySparseVSM, 'SparseVSM' ,parameters, dataset_name, True)\n",
    "               \n",
    "    #Processing low-dimensional representations\n",
    "    if config['low-dimension_representation']['use'] == True:\n",
    "        for type_repr in config['low-dimension_representation']['types']: \n",
    "            if type_repr == 'doc2vec': \n",
    "                print('=============================================')\n",
    "                print('Doc2Vec')\n",
    "                for model in config['low-dimension_representation']['doc2vec_config']['models']:\n",
    "                    print('Model:', model)\n",
    "                    for method in config['low-dimension_representation']['doc2vec_config']['methods']: \n",
    "                        print('Method:', method)\n",
    "                        for num_max_epoch in config['low-dimension_representation']['doc2vec_config']['num_max_epochs']:\n",
    "                            print('Num. Max Epochs:', num_max_epoch)\n",
    "                            for window_size in config['low-dimension_representation']['doc2vec_config']['window_sizes']:\n",
    "                                print('Window Size:', window_size)\n",
    "                                for num_dimensions in config['low-dimension_representation']['doc2vec_config']['num_dimensions']:\n",
    "                                    print('Num. Dimensions:', num_dimensions)\n",
    "                                    parameters = {'model' : model, 'method' : method, 'dim_size': num_dimensions,\n",
    "                                                  'num_max_epochs' : num_max_epoch, 'window_size' : window_size, \n",
    "                                                  'num_threads' : config['low-dimension_representation']['doc2vec_config']['num_threads'],\n",
    "                                                  'min_count' : config['low-dimension_representation']['doc2vec_config']['min_count'],\n",
    "                                                  'alpha' : config['low-dimension_representation']['doc2vec_config']['alpha'],\n",
    "                                                  'min_alpha' : config['low-dimension_representation']['doc2vec_config']['min_alpha']\n",
    "                                                  }\n",
    "                \n",
    "                                    myDoc2Vec = MyDoc2Vec(**parameters)\n",
    "                                    build_and_save_representation(config, myDoc2Vec, 'Doc2Vec', parameters, dataset_name, False)\n",
    "            elif type_repr == 'word2vec': \n",
    "                pass\n",
    "            else: \n",
    "                raise ValueError('Unsuported low dimension representation type. Please, use types = {\"doc2vec\",\"word2vec\"}.')\n",
    "            \n",
    "             \n",
    "\n",
    "print('Process Concluded!!')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando o resultado com o Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR = LogisticRegression()\n",
    "clfMNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8"
     },
     "metadata": {},
     "execution_count": 402
    }
   ],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split(representation.text_vectors, representation.class_vectors, test_size=0.20)\n",
    "train_X, test_X, train_y, test_y = train_test_split(representation.text_vectors, representation.class_vectors, test_size=0.20)\n",
    "clfLR.fit(train_X, train_y)\n",
    "clfLR.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.FastText(list_tokens_texts,min_count=5,window=5, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<gensim.models.fasttext.FastText at 0x7fb89ad3fd10>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2vecrepr = []\n",
    "for i in range(model.corpus_count): \n",
    "    doc2vecrepr.append(model.docvecs[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.75"
     },
     "metadata": {},
     "execution_count": 273
    }
   ],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split(representation.text_vectors, representation.class_vectors, test_size=0.20)\n",
    "train_X, test_X, train_y, test_y = train_test_split(doc2vecrepr, representation.class_vectors, test_size=0.20)\n",
    "clfLR.fit(train_X, train_y)\n",
    "clfLR.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'the strip bat be hang on -PRON- foot for good'"
     },
     "metadata": {},
     "execution_count": 288
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join','.join(df['class'].unique())\n",
    "\" \".join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}